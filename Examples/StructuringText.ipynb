{
  "cells": [
    {
      "metadata": {
        "id": "70070f81d04710f6"
      },
      "cell_type": "markdown",
      "source": [
        "## Structuring Text for Financial Analysis\n",
        "\n",
        "This notebook demonstrates several techniques for analyzing financial text documents:\n",
        "1. **Bag of Words** - Simple word frequency analysis\n",
        "2. **Sentiment Analysis** - Dictionary-based sentiment scoring\n",
        "3. **Embeddings** - Semantic similarity using sentence transformers\n",
        "4. **LLM Labelling** - Using large language models to classify text chunks\n"
      ],
      "id": "70070f81d04710f6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries & load dictionary\n",
        "!pip install PyPDF2\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "-U5ax0tF4oZK",
        "outputId": "343cfb4c-bdd3-48d8-de5a-c64c626dae38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-U5ax0tF4oZK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-10T04:45:31.509018800Z",
          "start_time": "2026-02-10T04:45:31.431325700Z"
        },
        "id": "cfc0eff0e9f5cb40"
      },
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PyPDF2 import PdfReader\n",
        "import pandas as pd\n",
        "\n",
        "def load_pdf_from_url(pdf_url: str) -> str:\n",
        "    \"\"\"\n",
        "    Download a PDF from a URL and return extracted text.\n",
        "\n",
        "    Args:\n",
        "        pdf_url: URL pointing to a PDF file\n",
        "\n",
        "    Returns:\n",
        "        Extracted text content from all pages\n",
        "    \"\"\"\n",
        "    print(f\"Downloading PDF from {pdf_url}...\")\n",
        "    response = requests.get(pdf_url)\n",
        "    if response.status_code != 200:\n",
        "        raise RuntimeError(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
        "\n",
        "    reader = PdfReader(BytesIO(response.content))\n",
        "    return \"\\n\".join((page.extract_text() or \"\") for page in reader.pages)\n"
      ],
      "id": "cfc0eff0e9f5cb40",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-10T04:45:37.361064Z",
          "start_time": "2026-02-10T04:45:31.728922300Z"
        },
        "id": "3d34a4454dbaab25",
        "outputId": "a9be6f7b-ccb3-4aec-b009-2e053e290ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Define URLs for Google earnings transcripts\n",
        "pdf_urls = {\n",
        "    \"goog_2025q4\": \"https://raw.githubusercontent.com/travlake/ai-investments-course/main/Examples/Alphabet_2025_Q4_Earnings_Transcript.pdf\",\n",
        "    \"goog_2025q3\": \"https://raw.githubusercontent.com/travlake/ai-investments-course/main/Examples/Alphabet_2025_Q3_Earnings_Transcript.pdf\",\n",
        "    \"goog_2023q1\": \"https://raw.githubusercontent.com/travlake/ai-investments-course/main/Examples/Alphabet_2023_Q1_Earnings_Transcript.pdf\"\n",
        "}\n",
        "\n",
        "# Download and extract text from all PDFs\n",
        "pdf_texts = {name: load_pdf_from_url(url) for name, url in pdf_urls.items()}\n",
        "\n",
        "print(f\"\\nLoaded {len(pdf_texts)} transcripts:\")\n",
        "for name, text in pdf_texts.items():\n",
        "    print(f\"  - {name}: {len(text):,} characters\")\n"
      ],
      "id": "3d34a4454dbaab25",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PDF from https://raw.githubusercontent.com/travlake/ai-investments-course/main/Examples/Alphabet_2025_Q4_Earnings_Transcript.pdf...\n",
            "Downloading PDF from https://raw.githubusercontent.com/travlake/ai-investments-course/main/Examples/Alphabet_2025_Q3_Earnings_Transcript.pdf...\n",
            "Downloading PDF from https://raw.githubusercontent.com/travlake/ai-investments-course/main/Examples/Alphabet_2023_Q1_Earnings_Transcript.pdf...\n",
            "\n",
            "Loaded 3 transcripts:\n",
            "  - goog_2025q4: 74,269 characters\n",
            "  - goog_2025q3: 66,783 characters\n",
            "  - goog_2023q1: 54,144 characters\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "dd4a3460d2a17437"
      },
      "cell_type": "markdown",
      "source": [
        "## Bag of Words Analysis\n",
        "A simple but effective technique for understanding document content by counting word frequencies.\n"
      ],
      "id": "dd4a3460d2a17437"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-09T22:04:16.356063Z",
          "start_time": "2026-02-09T22:04:16.337902Z"
        },
        "id": "39d6025735c255c8",
        "outputId": "dc58b327-13b0-49f7-9c34-a8991f49e277",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Analyze a single document - 2023 Q1 earnings call\n",
        "pdf_text = pdf_texts['goog_2023q1']\n",
        "\n",
        "# Create bag of words representation (excluding common English stop words)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform([pdf_text])\n",
        "\n",
        "# Extract word frequencies\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "word_counts = X.toarray().sum(axis=0)\n",
        "\n",
        "# Create DataFrame and show top 10 most frequent words\n",
        "word_freq_df = pd.DataFrame({'Word': feature_names, 'Count': word_counts})\n",
        "top_words = word_freq_df.sort_values(by='Count', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 Most Frequent Words (2023 Q1):\")\n",
        "print(top_words.to_string(index=False))\n"
      ],
      "id": "39d6025735c255c8",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Words (2023 Q1):\n",
            "   Word  Count\n",
            " google     66\n",
            "     ai     65\n",
            "quarter     49\n",
            " search     44\n",
            "youtube     42\n",
            "  think     42\n",
            "  cloud     38\n",
            " growth     36\n",
            "   year     33\n",
            "  thank     32\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-09T22:11:00.985232Z",
          "start_time": "2026-02-09T22:11:00.963001Z"
        },
        "id": "b9d64356f9f52458",
        "outputId": "912d2488-067d-4491-d510-cd55b4a361e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare word frequencies between two earnings calls\n",
        "pdf_text1 = pdf_texts['goog_2023q1']\n",
        "pdf_text2 = pdf_texts['goog_2025q4']\n",
        "\n",
        "# Fit vectorizer on first document, transform both\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X1 = vectorizer.fit_transform([pdf_text1])\n",
        "X2 = vectorizer.transform([pdf_text2])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Calculate word counts for each document\n",
        "word_counts1 = X1.toarray().sum(axis=0)\n",
        "word_counts2 = X2.toarray().sum(axis=0)\n",
        "\n",
        "# Create DataFrames\n",
        "word_freq_df1 = pd.DataFrame({'Word': feature_names, 'Count_2023': word_counts1})\n",
        "word_freq_df2 = pd.DataFrame({'Word': feature_names, 'Count_2025': word_counts2})\n",
        "\n",
        "# Calculate difference in word frequencies\n",
        "merged_df = word_freq_df1.merge(word_freq_df2, on='Word')\n",
        "merged_df['Diff'] = merged_df['Count_2025'] - merged_df['Count_2023']\n",
        "\n",
        "# Get top words for each period\n",
        "top_2023 = merged_df.nlargest(10, 'Count_2023')[['Word', 'Count_2023']].reset_index(drop=True)\n",
        "top_2025 = merged_df.nlargest(10, 'Count_2025')[['Word', 'Count_2025']].reset_index(drop=True)\n",
        "\n",
        "# Get rising and falling words (biggest changes)\n",
        "rising = merged_df.nlargest(10, 'Diff')[['Word', 'Count_2023', 'Count_2025', 'Diff']].reset_index(drop=True)\n",
        "falling = merged_df.nsmallest(10, 'Diff')[['Word', 'Count_2023', 'Count_2025', 'Diff']].reset_index(drop=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"WORD FREQUENCY COMPARISON: 2023 Q1 vs 2025 Q4\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nðŸ“Š Top 10 Words in 2023 Q1:\")\n",
        "print(top_2023.to_string(index=False))\n",
        "\n",
        "print(\"\\nðŸ“Š Top 10 Words in 2025 Q4:\")\n",
        "print(top_2025.to_string(index=False))\n",
        "\n",
        "print(\"\\nðŸ“ˆ Top 10 Rising Words (increased from 2023 to 2025):\")\n",
        "print(rising.to_string(index=False))\n",
        "\n",
        "print(\"\\nðŸ“‰ Top 10 Falling Words (decreased from 2023 to 2025):\")\n",
        "print(falling.to_string(index=False))\n"
      ],
      "id": "b9d64356f9f52458",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "WORD FREQUENCY COMPARISON: 2023 Q1 vs 2025 Q4\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Top 10 Words in 2023 Q1:\n",
            "   Word  Count_2023\n",
            " google          66\n",
            "     ai          65\n",
            "quarter          49\n",
            " search          44\n",
            "  think          42\n",
            "youtube          42\n",
            "  cloud          38\n",
            " growth          36\n",
            "   year          33\n",
            "  thank          32\n",
            "\n",
            "ðŸ“Š Top 10 Words in 2025 Q4:\n",
            "   Word  Count_2025\n",
            "     ai          94\n",
            "billion          55\n",
            "   year          53\n",
            "youtube          48\n",
            " google          47\n",
            " growth          43\n",
            "  think          43\n",
            " search          37\n",
            "quarter          34\n",
            "  cloud          32\n",
            "\n",
            "ðŸ“ˆ Top 10 Rising Words (increased from 2023 to 2025):\n",
            "       Word  Count_2023  Count_2025  Diff\n",
            "    billion          24          55    31\n",
            "         ai          65          94    29\n",
            "     strong          11          31    20\n",
            "       year          33          53    20\n",
            "       like          11          29    18\n",
            "       mode           2          19    17\n",
            "  increased           2          17    15\n",
            "     driven           3          16    13\n",
            "investments           4          17    13\n",
            "    revenue          12          25    13\n",
            "\n",
            "ðŸ“‰ Top 10 Falling Words (decreased from 2023 to 2025):\n",
            "   Word  Count_2023  Count_2025  Diff\n",
            "   ruth          22           0   -22\n",
            " google          66          47   -19\n",
            "   help          19           2   -17\n",
            " shorts          21           4   -17\n",
            " number          24           8   -16\n",
            "     ll          28          13   -15\n",
            "quarter          49          34   -15\n",
            "   2023          13           0   -13\n",
            "   cost          14           2   -12\n",
            "   long          20           8   -12\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2d5658af8754a618"
      },
      "cell_type": "markdown",
      "source": [
        "## Dictionary-Based Sentiment Analysis\n",
        "Use NLTK's VADER sentiment analyzer to measure positive, negative, and neutral sentiment in the transcripts.\n"
      ],
      "id": "2d5658af8754a618"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-09T22:42:27.665271Z",
          "start_time": "2026-02-09T22:42:27.500295Z"
        },
        "id": "c293037ac4ca257a",
        "outputId": "60f20765-752c-46f4-afac-aadbf16fa1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Analyze both transcripts\n",
        "pdf_text1 = pdf_texts['goog_2023q1']\n",
        "pdf_text2 = pdf_texts['goog_2025q4']\n",
        "\n",
        "sentiment1 = sia.polarity_scores(pdf_text1)\n",
        "sentiment2 = sia.polarity_scores(pdf_text2)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "sentiment_df = pd.DataFrame({\n",
        "    'Sentiment': ['Positive', 'Negative', 'Neutral', 'Compound'],\n",
        "    '2023 Q1': [sentiment1['pos'], sentiment1['neg'], sentiment1['neu'], sentiment1['compound']],\n",
        "    '2025 Q4': [sentiment2['pos'], sentiment2['neg'], sentiment2['neu'], sentiment2['compound']]\n",
        "})\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"SENTIMENT COMPARISON: 2023 Q1 vs 2025 Q4\")\n",
        "print(\"=\" * 50)\n",
        "print(sentiment_df.round(3).to_string(index=False))\n",
        "print(\"\\nNote: Compound score ranges from -1 (most negative) to +1 (most positive)\")\n"
      ],
      "id": "c293037ac4ca257a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "SENTIMENT COMPARISON: 2023 Q1 vs 2025 Q4\n",
            "==================================================\n",
            "Sentiment  2023 Q1  2025 Q4\n",
            " Positive    0.159    0.147\n",
            " Negative    0.008    0.009\n",
            "  Neutral    0.833    0.844\n",
            " Compound    1.000    1.000\n",
            "\n",
            "Note: Compound score ranges from -1 (most negative) to +1 (most positive)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-10T03:36:07.086266300Z",
          "start_time": "2026-02-10T03:36:07.023638700Z"
        },
        "id": "96867e71ef63ce59"
      },
      "cell_type": "markdown",
      "source": [
        "## Embeddings and Semantic Similarity\n",
        "Use sentence transformers to create dense vector representations (embeddings) of the documents. This allows us to measure semantic similarity between texts.\n"
      ],
      "id": "96867e71ef63ce59"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-10T05:03:22.092314600Z",
          "start_time": "2026-02-10T05:03:18.408927300Z"
        },
        "id": "45a5e2aa6be62626"
      },
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def chunk_text(text, chunk_size=100):\n",
        "    \"\"\"\n",
        "    Split text into chunks of approximately chunk_size words.\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "        chunk_size: Number of words per chunk\n",
        "\n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    return [\n",
        "        \" \".join(words[i:i+chunk_size])\n",
        "        for i in range(0, len(words), chunk_size)\n",
        "    ]\n",
        "\n",
        "# Create document embeddings by averaging chunk embeddings\n",
        "chunks_2025 = chunk_text(pdf_texts['goog_2025q4'])\n",
        "E_2025 = model.encode(chunks_2025, show_progress_bar=False)\n",
        "call_embedding_2025 = E_2025.mean(axis=0)\n",
        "\n",
        "chunks_2023 = chunk_text(pdf_texts['goog_2023q1'])\n",
        "E_2023 = model.encode(chunks_2023, show_progress_bar=False)\n",
        "call_embedding_2023 = E_2023.mean(axis=0)\n",
        "\n",
        "print(f\"Created embeddings with {len(call_embedding_2025)} dimensions\")\n",
        "print(f\"2025 Q4: {len(chunks_2025)} chunks\")\n",
        "print(f\"2023 Q1: {len(chunks_2023)} chunks\")\n"
      ],
      "id": "45a5e2aa6be62626",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-10T05:17:56.088370400Z",
          "start_time": "2026-02-10T05:17:56.020911Z"
        },
        "id": "419061d325e5ab7e",
        "outputId": "fe3ddaba-4bf0-4735-8e96-918452252efe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare document embedding to reference sentences to gain intuition about what the embedding captures\n",
        "sentences = [\n",
        "    \"An earnings call transcript for Google's Q4 2025 earnings.\",\n",
        "    \"Google reported strong earnings growth and increased capex in AI infrastructure\",\n",
        "    \"Google reported weak earnings growth and reduced capex in AI infrastructure\",\n",
        "    \"A recipe for sourdough bread including fermentation steps.\",\n",
        "    \"Management provides concrete metrics, timelines, and operational details about performance.\",\n",
        "    \"Management uses optimistic language but avoids specific numbers or commitments.\",\n",
        "    \"The company discusses increased capital expenditures, infrastructure investment, and long-term capacity building.\",\n",
        "    \"The company discusses buybacks, dividends, and returning capital to shareholders.\",\n",
        "    \"Management expresses confidence and upside opportunities.\",\n",
        "    \"Management expresses caution and downside risks.\",\n",
        "    \"AI\",\n",
        "    \"Pizza\",\n",
        "    \"Billions of dollars\",\n",
        "    \"Harry Potter\"\n",
        "]\n",
        "\n",
        "# Encode reference sentences\n",
        "sentence_embeddings = model.encode(sentences, show_progress_bar=False)\n",
        "\n",
        "# Calculate similarities between 2025 Q4 document and reference sentences\n",
        "similarities = cosine_similarity([call_embedding_2025], sentence_embeddings)[0]\n",
        "\n",
        "# Calculate similarity between the two earnings calls\n",
        "similarity_between_calls = cosine_similarity([call_embedding_2025], [call_embedding_2023])[0][0]\n",
        "\n",
        "# Display results as a DataFrame\n",
        "similarity_df = pd.DataFrame({\n",
        "    'Reference Sentence': sentences + ['2023 Q1 Call Transcript'],\n",
        "    'Similarity to 2025 Q4': list(similarities) + [similarity_between_calls]\n",
        "})\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EMBEDDING SIMILARITY ANALYSIS\")\n",
        "print(\"Comparing 2025 Q4 earnings call embedding to reference sentences\")\n",
        "print(\"=\" * 80)\n",
        "print(similarity_df.round(3).to_string(index=False))\n"
      ],
      "id": "419061d325e5ab7e",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "EMBEDDING SIMILARITY ANALYSIS\n",
            "Comparing 2025 Q4 earnings call embedding to reference sentences\n",
            "================================================================================\n",
            "                                                                                               Reference Sentence  Similarity to 2025 Q4\n",
            "                                                       An earnings call transcript for Google's Q4 2025 earnings.                  0.487\n",
            "                                  Google reported strong earnings growth and increased capex in AI infrastructure                  0.663\n",
            "                                      Google reported weak earnings growth and reduced capex in AI infrastructure                  0.630\n",
            "                                                       A recipe for sourdough bread including fermentation steps.                  0.049\n",
            "                      Management provides concrete metrics, timelines, and operational details about performance.                  0.294\n",
            "                                  Management uses optimistic language but avoids specific numbers or commitments.                  0.222\n",
            "The company discusses increased capital expenditures, infrastructure investment, and long-term capacity building.                  0.485\n",
            "                                The company discusses buybacks, dividends, and returning capital to shareholders.                  0.361\n",
            "                                                        Management expresses confidence and upside opportunities.                  0.279\n",
            "                                                                 Management expresses caution and downside risks.                  0.157\n",
            "                                                                                                               AI                  0.378\n",
            "                                                                                                            Pizza                  0.070\n",
            "                                                                                              Billions of dollars                  0.316\n",
            "                                                                                          2023 Q1 Call Transcript                  0.950\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "16914a364885d785"
      },
      "cell_type": "markdown",
      "source": [
        "## LLM Labelling\n",
        "Use a large language model (Gemini) to classify text chunks based on how AI is framed in the content.\n"
      ],
      "id": "16914a364885d785"
    },
    {
      "metadata": {
        "id": "13dcd7f697c77b20"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import json\n",
        "\n",
        "# Initialize Gemini client\n",
        "# Note: Replace with your own API key\n",
        "client = genai.Client(api_key=\"YOUR KEY HERE\")\n",
        "\n",
        "# Define the classification prompt\n",
        "prompt = \"\"\"\n",
        "How is AI framed in this text?\n",
        "\n",
        "Choose the closest category:\n",
        "- revenue_driver\n",
        "- cost_or_capex\n",
        "- strategic_positioning\n",
        "- vague_hype\n",
        "- not_discussed\n",
        "\n",
        "Think out loud in your response before giving your final answer.\n",
        "\n",
        "When ready to give your final answer, write \"<start of final answer>\". Everything after this tag should be a JSON with two keys: \"label\" and \"confidence\". The \"label\" should be one of the categories above, and \"confidence\" should be an integer between 1 and 5 indicating how confident you are in your label.\n",
        "\n",
        "Text:\n",
        "\"\"\"\n",
        "\n",
        "def extract_json_after_tag(text, tag=\"<start of final answer>\"):\n",
        "    \"\"\"\n",
        "    Extract JSON object from LLM response after a specific tag.\n",
        "\n",
        "    Args:\n",
        "        text: LLM response text\n",
        "        tag: Marker indicating start of structured output\n",
        "\n",
        "    Returns:\n",
        "        Parsed JSON as dictionary, or None if parsing fails\n",
        "    \"\"\"\n",
        "    idx = text.lower().find(tag.lower())\n",
        "    if idx == -1:\n",
        "        return None\n",
        "\n",
        "    snippet = text[idx + len(tag):]\n",
        "    start = snippet.find(\"{\")\n",
        "    if start == -1:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        return json.JSONDecoder().raw_decode(snippet[start:])[0]\n",
        "    except:\n",
        "        return None\n"
      ],
      "id": "13dcd7f697c77b20"
    },
    {
      "metadata": {
        "id": "43a771ce9cea2f20",
        "outputId": "64fc613b-fd13-40c7-d552-ea5257304e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 19 chunks through LLM...\n",
            "------------------------------------------------------------\n",
            "âœ“ Chunk 0/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 1/18: strategic_positioning (confidence: 5)\n",
            "âœ“ Chunk 2/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 3/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 4/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 5/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 6/18: revenue_driver (confidence: 4)\n",
            "âœ“ Chunk 7/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 8/18: cost_or_capex (confidence: 5)\n",
            "âœ“ Chunk 9/18: strategic_positioning (confidence: 5)\n",
            "âœ“ Chunk 10/18: cost_or_capex (confidence: 5)\n",
            "âœ“ Chunk 11/18: strategic_positioning (confidence: 4)\n",
            "âœ“ Chunk 12/18: strategic_positioning (confidence: 4)\n",
            "âœ“ Chunk 13/18: strategic_positioning (confidence: 4)\n",
            "âœ“ Chunk 14/18: cost_or_capex (confidence: 5)\n",
            "âœ“ Chunk 15/18: strategic_positioning (confidence: 4)\n",
            "âœ“ Chunk 16/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 17/18: revenue_driver (confidence: 5)\n",
            "âœ“ Chunk 18/18: revenue_driver (confidence: 5)\n",
            "\n",
            "============================================================\n",
            "LLM LABELLING RESULTS\n",
            "============================================================\n",
            "\n",
            "Label Distribution:\n",
            "label\n",
            "revenue_driver           10\n",
            "strategic_positioning     6\n",
            "cost_or_capex             3\n",
            "\n",
            "Average Confidence by Label:\n",
            "label\n",
            "cost_or_capex            5.00\n",
            "revenue_driver           4.90\n",
            "strategic_positioning    4.33\n",
            "\n",
            "Full Results:\n",
            " chunk_number                 label  confidence\n",
            "            0        revenue_driver           5\n",
            "            1 strategic_positioning           5\n",
            "            2        revenue_driver           5\n",
            "            3        revenue_driver           5\n",
            "            4        revenue_driver           5\n",
            "            5        revenue_driver           5\n",
            "            6        revenue_driver           4\n",
            "            7        revenue_driver           5\n",
            "            8         cost_or_capex           5\n",
            "            9 strategic_positioning           5\n",
            "           10         cost_or_capex           5\n",
            "           11 strategic_positioning           4\n",
            "           12 strategic_positioning           4\n",
            "           13 strategic_positioning           4\n",
            "           14         cost_or_capex           5\n",
            "           15 strategic_positioning           4\n",
            "           16        revenue_driver           5\n",
            "           17        revenue_driver           5\n",
            "           18        revenue_driver           5\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Process chunks through the LLM\n",
        "chunks = chunk_text(pdf_texts['goog_2025q4'], chunk_size=500)\n",
        "results = []\n",
        "\n",
        "print(f\"Processing {len(chunks)} chunks through LLM...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-3-flash-preview\",\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.0,\n",
        "                system_instruction=\"You are an experienced financial analyst specializing in AI companies.\"\n",
        "            ),\n",
        "            contents=prompt + chunk\n",
        "        )\n",
        "\n",
        "        parsed = extract_json_after_tag(response.text)\n",
        "\n",
        "        if parsed:\n",
        "            results.append({\n",
        "                \"chunk_number\": i,\n",
        "                \"label\": parsed.get(\"label\"),\n",
        "                \"confidence\": parsed.get(\"confidence\")\n",
        "            })\n",
        "            print(f\"âœ“ Chunk {i}/{len(chunks)-1}: {parsed.get('label')} (confidence: {parsed.get('confidence')})\")\n",
        "        else:\n",
        "            results.append({\n",
        "                \"chunk_number\": i,\n",
        "                \"label\": \"PARSE_ERROR\",\n",
        "                \"confidence\": 0\n",
        "            })\n",
        "            print(f\"âœ— Chunk {i}/{len(chunks)-1}: Parsing failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Chunk {i}/{len(chunks)-1}: API error - {e}\")\n",
        "        results.append({\n",
        "            \"chunk_number\": i,\n",
        "            \"label\": \"API_ERROR\",\n",
        "            \"confidence\": 0\n",
        "        })\n",
        "\n",
        "# Create results DataFrame\n",
        "df_output = pd.DataFrame(results, columns=[\"chunk_number\", \"label\", \"confidence\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LLM LABELLING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show summary statistics\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(df_output['label'].value_counts().to_string())\n",
        "\n",
        "print(\"\\nAverage Confidence by Label:\")\n",
        "print(df_output.groupby('label')['confidence'].mean().round(2).to_string())\n",
        "\n",
        "print(\"\\nFull Results:\")\n",
        "print(df_output.to_string(index=False))"
      ],
      "id": "43a771ce9cea2f20"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}